{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b4f244-8aae-4bff-b67e-9f47be48b56e",
   "metadata": {},
   "source": [
    "# Global Diferential Privacy Assessement: Bivariate case\n",
    "\n",
    "In this notebook, we will explore further the bivariate case of the Global Differential Privacy. After running this notebook, you will be able to:\n",
    "- Understand the concept and applying Gaussian based Global Differential Privacy for bivariate case.\n",
    "\n",
    "We also run a monte carl simulation to validate the GDP implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897ccb54-de00-4c80-8054-a1eadc546cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, wget, shutil, math\n",
    "from math import sqrt\n",
    "from scipy.stats import pearsonr\n",
    "from itertools import combinations\n",
    "from rich import print as pprint\n",
    "import warnings, random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from scipy.stats import multivariate_normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14228e79-df2e-425d-84a7-d79f0d582403",
   "metadata": {},
   "source": [
    "# Bivariate Differential Privacy\n",
    "Here is an extension of the previous 1D (univariate) version to 2D data where we extend from Height to Weight. The rest of the setup is as previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "634f2940-e5df-455e-b845-6de814b49ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_dp_noise_cholesky(chol_factor, sensitivity, max_attempts=10000):\n",
    "\n",
    "    for attempt in range(max_attempts):\n",
    "        z = np.random.standard_normal(2)\n",
    "        noise = chol_factor @ z\n",
    "        if np.all(noise >= sensitivity):\n",
    "            return noise, True\n",
    "    \n",
    "    return noise, False\n",
    "\n",
    "def bivariate_dp(data, original_output):\n",
    "\n",
    "    loo_output = np.array([np.mean(np.delete(data,i,axis=0), 0) for i in range(data.shape[0])])\n",
    "\n",
    "    sensitivities = np.max(np.abs(loo_output - original_output), 1)\n",
    "    data_cov = np.cov(data)\n",
    "    loo_scale = np.std(loo_output, axis=0)\n",
    "\n",
    "    scale_factor1 = (2 * loo_scale[0]) / np.sqrt(data_cov[0, 0])\n",
    "    scale_factor2 = (2 * loo_scale[1]) / np.sqrt(data_cov[1, 1])\n",
    "    scale_matrix = np.diag([scale_factor1, scale_factor2])\n",
    "    noise_cov = scale_matrix @ data_cov @ scale_matrix\n",
    "    eigenvals = np.linalg.eigvals(noise_cov)\n",
    "    if np.min(eigenvals) <= 1e-10:\n",
    "        noise_cov += np.eye(2) * 1e-8\n",
    "\n",
    "    chol_factor = np.linalg.cholesky(noise_cov)\n",
    "    noise, success = generate_dp_noise_cholesky(chol_factor, sensitivities)\n",
    "    noise1, noise2 = noise[0], noise[1]\n",
    "    noisy_outputs = np.stack([original_output[0] + noise1, original_output[1] + noise2])\n",
    "    \n",
    "    return noisy_outputs, sensitivities\n",
    "\n",
    "def user_pipeline(data):\n",
    "    return np.array([np.mean(data, axis=1)]).T\n",
    "\n",
    "def iqr_bounds(data, axis=0):\n",
    "\n",
    "    data = np.asarray(data)\n",
    "    q1 = np.percentile(data, 25, axis=axis)\n",
    "    q3 = np.percentile(data, 75, axis=axis)\n",
    "    \n",
    "    iqr = q3 - q1\n",
    "    lower_bounds = q1 - 1.5 * iqr\n",
    "    upper_bounds = q3 + 1.5 * iqr\n",
    "    \n",
    "    return iqr, lower_bounds, upper_bounds\n",
    "\n",
    "def outlier_(value, lower_bounds, upper_bounds):\n",
    "\n",
    "    value = np.asarray(value)\n",
    "    lower_bounds = np.asarray(lower_bounds)\n",
    "    upper_bounds = np.asarray(upper_bounds)\n",
    "    \n",
    "    return (value < lower_bounds) | (value > upper_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca323f-ae23-4476-94b1-b5c69d043850",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "We use Monte Carlo simulations to show that the 2D results are differentially private.\n",
    "Since GDP depends on the influence of a single observation, we generate 1000 datasets of size 20, 40, 60, 80, and 100 with one injected outlier. For each replicate, the query is the 2D leave-one-out mean, which without noise would allow reconstruction of individual data points (see notebook 1). By adding multivariate noise, we prevent such reconstruction and make outlier identification impossible.\n",
    "\n",
    "We then evaluate:\n",
    "\n",
    "- The ability (or inability) to detect the outlier in 2D space.\n",
    "- The accuracy of reconstructed 2D means relative to sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08479b-38d6-4e67-94bf-144cd0047fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_mc_iteration_bivariate(MC, sample_sizes, correlations, outlier=False):\n",
    "    \n",
    "    id_dr = {size: np.zeros(size) for size in sample_sizes}  \n",
    "    reconstructed = {size: [] for size in sample_sizes}\n",
    "    sensitivities_ = {size: [] for size in sample_sizes}\n",
    "    error_ = {size: [] for size in sample_sizes}  \n",
    "\n",
    "    for mc in range(MC):\n",
    "        for correlation in correlations:\n",
    "            cov_matrix = np.array([[1.0, correlation], \n",
    "                                   [correlation, 1.0]])\n",
    "            means = np.array([0.0,0.0])\n",
    "            master_data = np.random.multivariate_normal(means, cov_matrix, max(sample_sizes)).T\n",
    "            for sample_size in sample_sizes:\n",
    "                data = master_data[:, :sample_size]\n",
    "                _, lower_b, _ = iqr_bounds(data)\n",
    "                if outlier:\n",
    "                    data[:,0] = [np.max(lower_b) + 10, np.mean(lower_b) + 10] \n",
    "                else: pass    \n",
    "                data = data.T\n",
    "                loo_data = np.array([np.delete(data,i,axis=0) for i in range(data.shape[0])])\n",
    "                ######## PIPELINE ########\n",
    "                noisy_outputs = []\n",
    "                sens_outputs = []\n",
    "                for loo_subset in loo_data:\n",
    "                    n, s = bivariate_dp(loo_subset.T, user_pipeline(loo_subset.T))\n",
    "                    noisy_outputs.append(n.flatten().T)\n",
    "                    sens_outputs.append(s.flatten().T)\n",
    "                noisy_outputs = np.array(noisy_outputs)  \n",
    "                coeff_mat = np.ones((sample_size, sample_size)) - np.eye(sample_size)\n",
    "                recons = []\n",
    "    \n",
    "                for dim in range(noisy_outputs.shape[1]): \n",
    "                    recons_dim = np.linalg.solve(coeff_mat, (sample_size - 1) * noisy_outputs[:, dim])\n",
    "                    recons.append(recons_dim)\n",
    "                recons = np.stack(recons, axis=1) \n",
    "                reconstructed[sample_size].append(recons)\n",
    "                sensitivities_[sample_size].append(sens_outputs)\n",
    "                recons_error = np.mean(np.linalg.norm(recons - data, axis=1))\n",
    "                error_[sample_size].append(recons_error)\n",
    "            \n",
    "                _, lower_b, upper_b = iqr_bounds(recons[:,0])\n",
    "                    \n",
    "                for idx in range(sample_size):\n",
    "                    if np.all(outlier_(np.array(recons[idx,0]), np.array(lower_b), np.array(upper_b))):\n",
    "                        id_dr[sample_size][idx] += 1        \n",
    "    return id_dr, reconstructed, sensitivities_, error_\n",
    "           \n",
    "MC = 1000\n",
    "subsample_sizes = [20, 50, 80, 100, 200]\n",
    "correlations_bivariate = [0.0, 0.4, 0.9]\n",
    "\n",
    "id_dr_OL, reconstructed_OL, sensitivities_OL, error_OL = run_single_mc_iteration_bivariate(MC, subsample_sizes, correlations_bivariate, outlier=True) \n",
    "id_dr, reconstructed, sensitivities_, error_ = run_single_mc_iteration_bivariate(MC, subsample_sizes, correlations_bivariate, outlier=False) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa5f17-e5fc-47aa-b09e-583a8c1402e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {\n",
    "    20: '#FF6B6B',\n",
    "    50: '#4ECDC4',\n",
    "    80: '#650021',\n",
    "    100: '#FFA07A',\n",
    "    200: '#98FB98',\n",
    " }\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for size in subsample_sizes:\n",
    "    ax.scatter(np.mean(sensitivities_[size],1)[:,0], np.mean(sensitivities_[size],1)[:,1], np.array(error_[size]),\n",
    "               color=color_map[size],\n",
    "               s=60, alpha=0.7, edgecolors='k',\n",
    "               label=f'n={size}')\n",
    "\n",
    "ax.set_xlabel(\"Sensitivity Weight\")\n",
    "ax.set_ylabel(\"Sensitivity Height\")\n",
    "ax.set_zlabel(\"MAE\")\n",
    "ax.set_title(\"Bivariate Sensitivity vs. Error\")\n",
    "ax.legend(title=\"Subsample Size\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb82ef-1ff0-4b2e-b050-2480bc2f4bd5",
   "metadata": {},
   "source": [
    "# Sensitivity Vs. MAE\n",
    "\n",
    "This 3D plot shows the relationship between bivariate sensitivity (for weight and height) and the reconstruction error across different sample sizes.\n",
    "\n",
    "- X-axis (Sensitivity Weight) and Y-axis (Sensitivity Height) show how sensitive each dimension is to the removal of one individual (leave-one-out).\n",
    "- Z-axis (Error) measures how much error there is when trying to reconstruct the data after noise is added.\n",
    "\n",
    "\n",
    "### What we see:\n",
    "\n",
    "- Smaller datasets, like n=20, have higher sensitivity and higher reconstruction error, meaning outliers have a bigger influence.\n",
    "- The same behaviour as 1D, as sample size increases, sensitivity in both dimensions shrinks, and reconstruction error stabilizes (clusters get tighter and lower).\n",
    "\n",
    "Overall, the plot shows that in the 2D case, larger datasets protect privacy better (lower sensitivity) while keeping reconstruction error bounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb49f21b-617e-4f7d-aa8b-d6a40ac9cef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef7af0c-b133-4956-9633-45f80be9e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sensitivities = list()\n",
    "std_errors = list()\n",
    "\n",
    "for size in subsample_sizes:\n",
    "    all_sensitivities = np.array(sensitivities_[size]).flatten()\n",
    "    mean_sens = np.mean(all_sensitivities)\n",
    "    std_err = np.std(all_sensitivities) / np.sqrt(len(all_sensitivities))\n",
    "    mean_sensitivities.append(mean_sens)\n",
    "    std_errors.append(std_err)\n",
    "    \n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.errorbar(\n",
    "    subsample_sizes, mean_sensitivities, yerr=std_errors,\n",
    "    fmt='o-', capsize=5, color='blue',\n",
    "    label='Mean Sensitivity +/- SE (NO OUTLIER)'\n",
    ")\n",
    "\n",
    "\n",
    "for size, sens, err in zip(subsample_sizes, mean_sensitivities, std_errors):\n",
    "    plt.annotate(\n",
    "        f\"{sens:.2f}+/-{err:.2f}\", \n",
    "        (size, sens),\n",
    "        textcoords=\"offset points\", \n",
    "        xytext=(0, 10),\n",
    "        ha='right', \n",
    "        fontsize=12,\n",
    "        color='blue'\n",
    "    )\n",
    "\n",
    "\n",
    "plt.xlabel('Sample Size', fontsize=12)\n",
    "plt.ylabel('Average Sensitivities of Weight & Height', fontsize=12)\n",
    "plt.title('Sensitivity vs. Sample Size (+/- Standard Error)', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(subsample_sizes)\n",
    "plt.legend(fontsize=10, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a23946-b865-4570-ab8b-53080822eb97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893ef61e-edbe-41a3-bd22-6167d64505d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8059c3-0296-4694-a93a-c74655e854c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "mean_sens_w, mean_sens_h = list(), list()\n",
    "std_err_w, std_err_h = list(), list()\n",
    "\n",
    "for size in subsample_sizes:\n",
    "    w_sens_all = np.array(sensitivities_[size])[0:,:,0].flatten()\n",
    "    h_sens_all = np.array(sensitivities_[size])[0:,:,1].flatten()\n",
    "    w_sens = w_sens_all.mean()\n",
    "    h_sens = h_sens_all.mean()\n",
    "    \n",
    "    w_std_err = np.std(w_sens_all) / np.sqrt(len(w_sens_all))\n",
    "    h_std_err = np.std(h_sens_all) / np.sqrt(len(h_sens_all))\n",
    "    \n",
    "    mean_sens_w.append(w_sens)\n",
    "    mean_sens_h.append(h_sens)\n",
    "    \n",
    "    std_err_w.append(w_std_err)\n",
    "    std_err_h.append(h_std_err)\n",
    "    \n",
    "plt.errorbar(\n",
    "    subsample_sizes, mean_sens_w, yerr=std_err_w,\n",
    "    fmt='o-', capsize=5, color='blue',\n",
    "    label='Mean Sensitivity +/- SE Weight'\n",
    ")\n",
    "for size, sens, err in zip(subsample_sizes, mean_sens_w, std_err_w):\n",
    "    plt.annotate(\n",
    "        f\"{sens:.4f}+/-{err:.4f}\", \n",
    "        (size, sens),\n",
    "        textcoords=\"offset points\", \n",
    "        xytext=(0, 10),\n",
    "        ha='right', \n",
    "        fontsize=12,\n",
    "        color='blue'\n",
    "    )\n",
    "\n",
    "plt.errorbar(\n",
    "    subsample_sizes, mean_sens_h, yerr=std_err_h,\n",
    "    fmt='o-', capsize=5, color='red',\n",
    "    label='Mean Sensitivity +/- SE Height'\n",
    ")  \n",
    "\n",
    "for size, sens, err in zip(subsample_sizes, mean_sens_h, std_err_h):\n",
    "    plt.annotate(\n",
    "        f\"{sens:.4f}+/-{err:.4f}\", \n",
    "        (size, sens),\n",
    "        textcoords=\"offset points\", \n",
    "        xytext=(0, 10),\n",
    "        ha='right', \n",
    "        fontsize=12,\n",
    "        color='red'\n",
    "    )\n",
    "plt.xlabel('Sample Size', fontsize=12)\n",
    "plt.ylabel('Seperated Sensitivities of Weight & Height', fontsize=12)\n",
    "plt.title('Sensitivity vs. Sample Size (+/- Standard Error)', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(subsample_sizes)\n",
    "plt.legend(fontsize=10, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58092ab6-4b86-4e03-9cfe-6b91d9de67c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436969bd-4647-4498-a175-4b925c5586cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302421f8-a203-4166-9ef0-810c74345b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sens1 = []\n",
    "mean_sens2 = []\n",
    "std_err1 = []\n",
    "std_err2 = []\n",
    "\n",
    "for size in subsample_sizes:\n",
    "    sens = np.array(sensitivities_[size]) \n",
    "    mean_sens1.append(np.mean(sens[:, 0]))\n",
    "    mean_sens2.append(np.mean(sens[:, 1]))\n",
    "    std_err1.append(np.std(sens[:, 0]) / np.sqrt(len(sens)))\n",
    "    std_err2.append(np.std(sens[:, 1]) / np.sqrt(len(sens)))\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.errorbar(\n",
    "    subsample_sizes, mean_sens1, yerr=std_err1,\n",
    "    fmt='o-', capsize=5, color='blue',\n",
    "    label='Mean Sensitivity Weight +/- SE'\n",
    ")\n",
    "\n",
    "plt.errorbar(\n",
    "    subsample_sizes, mean_sens2, yerr=std_err2,\n",
    "    fmt='s--', capsize=5, color='green',\n",
    "    label='Mean Sensitivity Height +/- SE'\n",
    ")\n",
    "\n",
    "for size, s1, e1, s2, e2 in zip(subsample_sizes, mean_sens1, std_err1, mean_sens2, std_err2):\n",
    "    plt.annotate(f\"{s1:.4f}+/-{e1:.4f}\", (size, s1),\n",
    "                 textcoords=\"offset points\", xytext=(0, 8),\n",
    "                 ha='center', fontsize=14, color='blue')\n",
    "    plt.annotate(f\"{s2:.4f}+/-{e2:.4f}\", (size, s2),\n",
    "                 textcoords=\"offset points\", xytext=(0, -12),\n",
    "                 ha='center', fontsize=14, color='green')\n",
    "\n",
    "plt.xlabel(\"Sample Size\", fontsize=12)\n",
    "plt.ylabel(\"Average Sensitivity\", fontsize=12)\n",
    "plt.title(\"Sensitivity vs. Sample Size +/- SE\", fontsize=14)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.xticks(subsample_sizes)\n",
    "plt.legend(fontsize=10, loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82781044-2d28-4336-9b7a-6be2e2de367d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f1e6a-a4c6-45ef-9555-d831f1678d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_rates = (np.array([id_dr[size][0] for size in subsample_sizes], dtype=object) / n_replicates ) * 100 \n",
    "prc = (np.array([id_dr[size][0] for size in subsample_sizes], dtype=object) / n_replicates ) * 100\n",
    "\n",
    "means = []\n",
    "errors = []\n",
    "pop_size = []\n",
    "\n",
    "for size in subsample_sizes:\n",
    "    percentages = (id_dr[size] / n_replicates) * 100\n",
    "    mean_val = np.mean(percentages)\n",
    "    se_val = np.std(percentages, ddof=1) / np.sqrt(len(percentages))\n",
    "    means.append(mean_val)\n",
    "    errors.append(se_val)\n",
    "    pop_size.append(size)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(subsample_sizes, detection_rates, \n",
    "         marker='o', linestyle='-', color='blue', label='Index 0', linewidth=2, markersize=8)\n",
    "\n",
    "for size in subsample_sizes:\n",
    "    if size in detection_rates:\n",
    "        rate = detection_rates[size]\n",
    "        plt.annotate(f\"{rate:.4f}%\", (size, rate),\n",
    "                     textcoords=\"offset points\", xytext=(0, 10),\n",
    "                     ha='right', fontsize=12, color='blue')\n",
    "\n",
    "plt.errorbar(pop_size, means, yerr=errors, fmt='-o', capsize=5, \n",
    "             color='red', label='Sample', linewidth=2, markersize=8, elinewidth=2)\n",
    "\n",
    "for x, y, err in zip(pop_size, means, errors):\n",
    "    plt.annotate(f\"{y:.4f}% Â± {err:.4f}\",\n",
    "                 xy=(x, y), xytext=(0, 10),\n",
    "                 textcoords='offset points', ha='left',\n",
    "                 fontsize=12, color='red',\n",
    "                 bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.xlabel('Sample Size', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Outlier detection rate (%)', fontsize=12, fontweight='bold')\n",
    "plt.title(f'Outlier detection rates over {n_replicates} runs', fontsize=14, pad=20)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xticks(subsample_sizes)\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c95fd-f70c-4bdb-8d46-7536abaec193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e502d38-2ede-4df9-b80a-defee8e2eb17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
