{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897ccb54-de00-4c80-8054-a1eadc546cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, wget, shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f45e8f2-6752-4a4f-b98a-fc23a159f417",
   "metadata": {},
   "source": [
    "# Notebook overview\n",
    "\n",
    "## Differential Privacy and Privacy Protection\n",
    "Differential Privacy (DP) is a mathematical framework that provides strong, quantifiable privacy guarantees when analyzing and sharing data. Its core idea is to ensure that the inclusion or exclusion of a single individual’s data does not significantly affect the output of an analysis, thereby limiting what can be inferred about any one person. This limits the ability of an attacker to infer whether someone participated in the dataset, even if they possess additional background information. To achieve this, DP algorithms inject carefully calibrated random noise into computations, such as LOO mean, making it mathematically difficult to isolate individual contributions. \n",
    "\n",
    "In this section we show how DP can be applied to data. This example revolves around anonymizing a dataset of individual heights that contains significant outliers. \n",
    "\n",
    "## The goal\n",
    "\n",
    "The goal is to protect the data without compromising the privacy of any individual, especially in high risk or adversarial environments.\n",
    "\n",
    "Down below we use usecase-2.1 individual's height.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d619ca2-2428-426a-a3c7-bdc0a36ad8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..........................................................] 39886 / 39886\n",
      "Original file downloaded.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "link_original = \"https://s3.amazonaws.com/openneuro.org/ds004148/participants.tsv?versionId=wt81Mu2B3fdeiXSis5ym288A64lXRXkR\"\n",
    "wget.download(link_original)\n",
    "filename = \"participants.tsv\"\n",
    "file_ = [os.path.join(root, file) for root, _, files in os.walk(os.getcwd()) for file in files if file == filename]\n",
    "shutil.copy2(file_[0], \"data\")\n",
    "os.remove(file_[0])\n",
    "print(f\"\\nOriginal file downloaded.\")\n",
    "\n",
    "data = np.array(pd.read_csv(\"data/\"+filename,delim_whitespace=\"\\t\")[\"Height\"]).reshape(-1,1)\n",
    "clean_data = data[~np.isnan(data)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af07bb7-6bb4-4b66-98ca-762981614e7f",
   "metadata": {},
   "source": [
    "## User Output\n",
    "\n",
    "A data user on the SIESTA platform is typically a researcher from an institution, aiming to answer a scientific question using sensitive data. Since the original data cannot be downloaded directly due to privacy concerns, the data user must develop their analysis pipeline using an anonymized dataset that mimics the structure of the real data. Once developed, the analysis is containerized, where this containerized pipeline is then executed by the platform operator on the original data. \n",
    "\n",
    "After applying **differential privacy** techniques on the results, it will be shared back with the data user. If the result is injected with noise before releasing it, for example, by adding Laplace noise proportional to the L1 sensitivity of the mean. The attacker cannot recover the true values exactly, because the values are altered with noise. \n",
    "\n",
    "We implement a DP mechanism by adding Laplace noise scaled by sensitivity, defined here as the maximum deviation between the true mean and the leave-one-out estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b243a5b-f7ea-41ce-a372-6293446191da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a116faa4-8c61-47da-b7f5-024670fe85a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163.2050190061307"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def user_output(clean_data):\n",
    "\n",
    "    pipeline_output = np.mean(clean_data)\n",
    "    return pipeline_output\n",
    "\n",
    "def dp(clean_data,pipeline_output):\n",
    "\n",
    "    user_output_variable = np.array([user_output(np.delete(clean_data, i)) for i in range(len(clean_data))])\n",
    "    \n",
    "    loo_mean = np.mean(user_output_variable)\n",
    "    loo_scale = np.std(user_output_variable)\n",
    "    sensitivity = np.max(np.abs(loo_mean - pipeline_output)) \n",
    "\n",
    "    while True:\n",
    "        noise = np.random.laplace(loc=0.0, scale=np.median(loo_scale))\n",
    "        if abs(noise) >= sensitivity:\n",
    "            break\n",
    "        \n",
    "    return pipeline_output + noise\n",
    "\n",
    "pipeline_output = user_output(clean_data)\n",
    "dp(clean_data,pipeline_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0c18c-fe10-4c78-b021-45802bcea42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11d6c903-8a30-4d55-b4d9-87e6f6570acc",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "Here we simulate a potential privacy attack scenario where an adversary attempts to reconstruct the original individual data values from noisy LOO means.\n",
    "\n",
    "To protect privacy, Laplace noise is added to the LOO means using the DP mechanism. However, even after adding noise, it's important to evaluate how well the attacker could still reconstruct the original values. \n",
    "\n",
    "The DP mechanism is randomized, each execution adds different noise. So, we run a Monte Carlo (MC) simulation 1000 times to:\n",
    "\n",
    "- Simulate many possible outcomes of the noisy LOO release.\n",
    "- Perform reconstruction attempts based on each noisy release.\n",
    "- Visualize the distribution of attacker guesses for each individual’s value.\n",
    "\n",
    "This allows us to:\n",
    "\n",
    "- Understand the variability in the reconstructions.\n",
    "- Check whether the true values are consistently hidden within a wide distribution.\n",
    "- Provide empirical evidence that the data remains private on average, even if some reconstructions appear close by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e390452d-b16b-42e1-b2ab-1e659fa74917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
