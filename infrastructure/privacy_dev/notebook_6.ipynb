{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20358221-95c7-4117-8366-ccdf517f2a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from joblib import Parallel, delayed\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbc1ab9c-c215-4ef9-a546-7541e3be3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_pipeline(data):\n",
    "    return np.mean(data, axis=1, keepdims=True)\n",
    "\n",
    "@jit(nopython=True)\n",
    "def loo_(data):\n",
    "    n_features, n_samples = data.shape\n",
    "    loo_means = np.zeros((n_features, n_samples))\n",
    "    loo_stds = np.zeros((n_features, n_samples))\n",
    "\n",
    "    for i in range(n_features):\n",
    "        v = data[i]\n",
    "        s, s2 = np.sum(v), np.sum(v**2)\n",
    "        means = (s - v) / (n_samples - 1)\n",
    "        vars_ = ((s2 - v**2) - (n_samples - 1) * means**2) / (n_samples - 2)\n",
    "        loo_means[i], loo_stds[i] = means, np.sqrt(vars_)\n",
    "\n",
    "    return loo_means, loo_stds\n",
    "\n",
    "@jit(nopython=True)\n",
    "def gen_noise(chol_factor, sensitivity, max_attempts=10000):\n",
    "    dim = sensitivity.shape[0]\n",
    "    for _ in range(max_attempts):\n",
    "        z = np.random.standard_normal(dim)\n",
    "        noise = chol_factor @ z\n",
    "        if np.all(noise >= sensitivity):\n",
    "            return noise, True\n",
    "    return noise, False\n",
    "\n",
    "def dp(data, original_output, epsilon=1.0):\n",
    "    loo_means, loo_stds = loo_(data)\n",
    "    sensitivity = np.max(np.abs(loo_means - original_output), axis=1)\n",
    "\n",
    "    loo_scales = np.std(loo_stds, axis=1) / epsilon\n",
    "    cov = np.cov(data)\n",
    "    scale_factors = 2 * loo_scales / np.sqrt(np.diag(cov))\n",
    "    scale_matrix = np.diag(scale_factors)\n",
    "\n",
    "    noise_cov = scale_matrix @ cov @ scale_matrix\n",
    "    if np.min(np.linalg.eigvals(noise_cov)) <= 1e-10:\n",
    "        noise_cov += np.eye(data.shape[0]) * 1e-8\n",
    "\n",
    "    try:\n",
    "        chol = np.linalg.cholesky(noise_cov)\n",
    "        noise, success = gen_noise(chol, sensitivity)\n",
    "        if not success:\n",
    "            raise ValueError(\"Correlated noise generation failed\")\n",
    "    except (np.linalg.LinAlgError, ValueError):\n",
    "        warnings.warn(\"Failed to generate correlated noise, using independent noise\")\n",
    "        noise = np.array([np.random.normal(0, 2 * s) for s in loo_scales])\n",
    "        for i in range(len(noise)):\n",
    "            while abs(noise[i]) < sensitivity[i]:\n",
    "                noise[i] = np.random.normal(0, 2 * loo_scales[i])\n",
    "\n",
    "    noisy_outputs = original_output.flatten() + noise\n",
    "    return noisy_outputs, sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7dbd6a-09cb-49d1-965a-be7c746167e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f5fd56-39c3-4928-a439-d51f862bbd37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38662bb8-c64d-4028-8e85-10512ab02e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c78f2bd-bfba-4c5d-802f-27974568e9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time taken size [3,20000]: 00:00:02\n",
      "\n",
      "Total time taken size [400,50000]: 00:00:03\n",
      "\n",
      "Total time taken size [248, 245000]: 00:00:06\n"
     ]
    }
   ],
   "source": [
    "def main(data):\n",
    "    original_output = user_pipeline(data)\n",
    "    with warnings.catch_warnings(record=True) as w:\n",
    "        warnings.simplefilter(\"always\")\n",
    "        noisy_outputs, sensitivities = dp(data, original_output)\n",
    "        failed = any(\"Failed to generate correlated noise\" in str(msg.message) for msg in w)\n",
    "\n",
    "    result = {f'dp_t{i+1}': noisy_outputs[i] for i in range(len(noisy_outputs))}\n",
    "    result.update({f'sensitivity_t{i+1}': sensitivities[i] for i in range(len(sensitivities))})\n",
    "    return result, int(failed)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #cov_matrix = np.array([[1.0, 0.5], [0.5, 1.0]])\n",
    "    #data = np.random.multivariate_normal([0, 0], cov_matrix, 20000).T\n",
    "    \n",
    "    cov_matrix = np.array([[1.0, 0.5, 0.3],\n",
    "                           [0.5, 1.0, 0.2],\n",
    "                           [0.3, 0.2, 1.0]])\n",
    "    data = np.random.multivariate_normal([0, 0, 0], cov_matrix, 20000).T\n",
    "    import time\n",
    "    s = time.time()\n",
    "    results = Parallel(n_jobs=20, verbose=0, batch_size=1)(\n",
    "        delayed(main)(data) for _ in range(2)\n",
    "    )\n",
    "    total_time = time.time() - s\n",
    "    print(f\"\\nTotal time taken size [3,20000]: {time.strftime('%H:%M:%S', time.gmtime(total_time))}\")\n",
    "    size =  [400,50000]\n",
    "    s = time.time()\n",
    "    data = np.random.normal(loc=0.0, scale=1.0, size=size)\n",
    "    results = Parallel(n_jobs=20, verbose=0, batch_size=1)(\n",
    "            delayed(main)(data) for _ in range(2)\n",
    "        )\n",
    "    total_time = time.time() - s\n",
    "    print(f\"\\nTotal time taken size [400,50000]: {time.strftime('%H:%M:%S', time.gmtime(total_time))}\")\n",
    "\n",
    "    size =  np.array([248, 70 * 70 * 50])\n",
    "    s = time.time()\n",
    "    data = np.random.normal(loc=0.0, scale=1.0, size=size)\n",
    "    results = Parallel(n_jobs=20, verbose=0, batch_size=1)(\n",
    "            delayed(main)(data) for _ in range(2)\n",
    "        )\n",
    "    total_time = time.time() - s\n",
    "    print(f\"\\nTotal time taken size [248, 245000]: {time.strftime('%H:%M:%S', time.gmtime(total_time))}\")\n",
    "    \"\"\"\n",
    "    size =  np.array([70 * 70 * 50, 248])\n",
    "    s = time.time()\n",
    "    data = np.random.normal(loc=0.0, scale=1.0, size=size)\n",
    "    results = Parallel(n_jobs=20, verbose=0, batch_size=1)(\n",
    "            delayed(main)(data) for _ in range(2)\n",
    "        )\n",
    "    total_time = time.time() - s\n",
    "    print(f\"\\nTotal time taken size [245000, 248]: {time.strftime('%H:%M:%S', time.gmtime(total_time))}\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064152c-c727-4d46-8929-37d4ea1b1b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14446360-9d84-41fc-b16f-2a559da91416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc50805-b906-43bc-8da1-5bf26cb142c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44a68d47-94c4-4be1-b3f6-06d82f1099e4",
   "metadata": {},
   "source": [
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6beb1ba-94e0-43fd-85de-ff0a10094a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, subprocess, wget, shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf7a719-1b48-4cf9-a12e-4eba7faafefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [....................................................] 60545196 / 60545196\n",
      "Original file downloaded.\n"
     ]
    }
   ],
   "source": [
    "script = os.path.join(os.path.dirname(os.getcwd())+\"/CalibrateNoise/\", \"main.py\") \n",
    "file = \"input/anat/original/\"\n",
    "clean = True\n",
    "\n",
    "os.makedirs(\"img\", exist_ok=True)\n",
    "link_original = \"https://s3.amazonaws.com/openneuro.org/ds004934/sub-SAXNES2s001/func/sub-SAXNES2s001_task-DOTS_run-001_bold.nii.gz?versionId=R0fwRS9fxw8CcPZnb4zYsw9I5v19aAbP\"\n",
    "wget.download(link_original)\n",
    "os.makedirs(file, exist_ok=True)\n",
    "file_ = [os.path.join(root, file) for root, _, files in os.walk(os.getcwd()) for file in files if file.endswith(\".gz\")]\n",
    "shutil.copy2(file_[0], file)\n",
    "os.remove(file_[0])\n",
    "print(f\"\\nOriginal file downloaded.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb201c85-bf7a-48d8-82f7-38920b976745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time taken: 00:00:05\n",
      "\n",
      "Cleaning up finished\n"
     ]
    }
   ],
   "source": [
    "result = subprocess.run(\n",
    "    [\"python3\", script, file],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "print(result.stdout)\n",
    "\n",
    "if clean:\n",
    "    ! rm -r input\n",
    "    print(f\"Cleaning up finished\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee85d2f0-d6ae-4052-a751-ae5536f16f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
